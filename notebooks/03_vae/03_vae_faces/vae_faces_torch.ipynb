{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b076bd1a-b236-4fbc-953d-8295b25122ae",
   "metadata": {},
   "source": [
    "# ü§™ ÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÔºàVAEÔºâ‚Äî‚ÄîCelebA ‰∫∫ËÑ∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9235cbd1-f136-411c-88d9-f69f270c0b96",
   "metadata": {},
   "source": [
    "Êú¨ notebook Â∞ÜÈÄêÊ≠•‰ªãÁªçÂ¶Ç‰ΩïÂú® CelebA ‰∫∫ËÑ∏Êï∞ÊçÆÈõÜ ‰∏äËÆ≠ÁªÉ‰Ω†Ëá™Â∑±ÁöÑ ÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÔºàVAEÔºâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84acc7be-6764-4668-b2bb-178f63deeed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils as vutils\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e6268-ebd7-4feb-86db-1fe7abccdbe5",
   "metadata": {},
   "source": [
    "## 0. Parameters <a name=\"parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b2ee6ce-129f-4833-b0c5-fa567381c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 128\n",
    "NUM_FEATURES = 128\n",
    "Z_DIM = 200\n",
    "LEARNING_RATE = 5e-4\n",
    "EPOCHS = 10\n",
    "BETA = 2000\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7716fac-0010-49b0-b98e-53be2259edde",
   "metadata": {},
   "source": [
    "## 1. Prepare the data <a name=\"prepare\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a73e5a4-1638-411c-8d3c-29f823424458",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑË∑ØÂæÑ„ÄÇ: './data/img_align_celeba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      2\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((IMAGE_SIZE, IMAGE_SIZE)),\n\u001b[0;32m      3\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),           \u001b[38;5;66;03m# [0,1]\u001b[39;00m\n\u001b[0;32m      4\u001b[0m ])\n\u001b[1;32m----> 6\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/img_align_celeba\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m     12\u001b[0m     train_dataset,\n\u001b[0;32m     13\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     17\u001b[0m )\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\mlp_torch\\lib\\site-packages\\torchvision\\datasets\\folder.py:328\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    321\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    326\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    327\u001b[0m ):\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\mlp_torch\\lib\\site-packages\\torchvision\\datasets\\folder.py:149\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    140\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[1;32m--> 149\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[0;32m    152\u001b[0m         class_to_idx\u001b[38;5;241m=\u001b[39mclass_to_idx,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    155\u001b[0m         allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[0;32m    156\u001b[0m     )\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\mlp_torch\\lib\\site-packages\\torchvision\\datasets\\folder.py:234\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\mlp_torch\\lib\\site-packages\\torchvision\\datasets\\folder.py:41\u001b[0m, in \u001b[0;36mfind_classes\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfind_classes\u001b[39m(directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑË∑ØÂæÑ„ÄÇ: './data/img_align_celeba'"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),           # [0,1]\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root=\"./data/celeba-dataset/img_align_celeba\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebae2f0d-59fd-4796-841f-7213eae638de",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ÊòæÁ§∫‰∏ÄÊâπ‰∫∫ËÑ∏\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m images, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mtrain_loader\u001b[49m))\n\u001b[0;32m      3\u001b[0m grid \u001b[38;5;241m=\u001b[39m vutils\u001b[38;5;241m.\u001b[39mmake_grid(images[:\u001b[38;5;241m16\u001b[39m], nrow\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m6\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# ÊòæÁ§∫‰∏ÄÊâπ‰∫∫ËÑ∏\n",
    "images, _ = next(iter(train_loader))\n",
    "grid = vutils.make_grid(images[:16], nrow=4)\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(grid.permute(1,2,0))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff50401-3abe-4c10-bba8-b35bc13ad7d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Build the variational autoencoder <a name=\"build\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0625b6-3c19-478b-84f9-5c2b5c2b74b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu, logvar):\n",
    "    eps = torch.randn_like(mu)\n",
    "    return mu + torch.exp(0.5 * logvar) * eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086e2584-c60d-4990-89f4-2092c44e023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, NUM_FEATURES, 3, 2, 1),\n",
    "            nn.BatchNorm2d(NUM_FEATURES),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(NUM_FEATURES, NUM_FEATURES, 3, 2, 1),\n",
    "            nn.BatchNorm2d(NUM_FEATURES),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(NUM_FEATURES, NUM_FEATURES, 3, 2, 1),\n",
    "            nn.BatchNorm2d(NUM_FEATURES),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2d(NUM_FEATURES, NUM_FEATURES, 3, 2, 1),\n",
    "            nn.BatchNorm2d(NUM_FEATURES),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_mu = nn.Linear(NUM_FEATURES * 2 * 2, Z_DIM)\n",
    "        self.fc_logvar = nn.Linear(NUM_FEATURES * 2 * 2, Z_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flatten(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        z = reparameterize(mu, logvar)\n",
    "        return mu, logvar, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88010f20-fb61-498c-b2b2-dac96f6c03b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(Z_DIM, NUM_FEATURES * 2 * 2),\n",
    "            nn.BatchNorm1d(NUM_FEATURES * 2 * 2),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(NUM_FEATURES, NUM_FEATURES, 3, 2, 1, 1),\n",
    "            nn.BatchNorm2d(NUM_FEATURES),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(NUM_FEATURES, NUM_FEATURES, 3, 2, 1, 1),\n",
    "            nn.BatchNorm2d(NUM_FEATURES),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(NUM_FEATURES, NUM_FEATURES, 3, 2, 1, 1),\n",
    "            nn.BatchNorm2d(NUM_FEATURES),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(NUM_FEATURES, CHANNELS, 3, 2, 1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        x = x.view(-1, NUM_FEATURES, 2, 2)\n",
    "        return self.deconv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ad9761-9756-45b3-83ef-ee3d9218d694",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar, z = self.encoder(x)\n",
    "        recon = self.decoder(z)\n",
    "        return mu, logvar, recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf2f892-9209-42ee-b251-1e7604df5335",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b14665-4359-447b-be58-3fd58ba69084",
   "metadata": {},
   "source": [
    "## 3. Train the variational autoencoder <a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda3037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x, recon, mu, logvar):\n",
    "    recon_loss = BETA * F.mse_loss(recon, x, reduction=\"mean\")\n",
    "    kl = -0.5 * torch.mean(\n",
    "        torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
    "    )\n",
    "    return recon_loss + kl, recon_loss, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5328e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    total = 0\n",
    "    for x, _ in train_loader:\n",
    "        x = x.to(DEVICE)\n",
    "        mu, logvar, recon = vae(x)\n",
    "        loss, r, kl = vae_loss(x, recon, mu, logvar)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: loss={total/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c06dcb-cd9c-4784-93f8-0cff7002cf49",
   "metadata": {},
   "source": [
    "## 4. Latent space distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1fece5-77a8-4510-be7d-713cc08aee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    x, _ = next(iter(train_loader))\n",
    "    x = x.to(DEVICE)\n",
    "    mu, logvar, z = vae.encoder(x)\n",
    "\n",
    "z = z.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7fba06-6a5f-49c2-82a7-e6265acf1477",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.linspace(-3, 3, 100)\n",
    "plt.figure(figsize=(20,5))\n",
    "for i in range(50):\n",
    "    plt.subplot(5,10,i+1)\n",
    "    plt.hist(z[:,i], bins=20, density=True)\n",
    "    plt.plot(x_axis, norm.pdf(x_axis))\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa68340-5b03-4307-8a0f-2fe2d1658846",
   "metadata": {},
   "source": [
    "## 5. Generate new faces <a name=\"decode\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8189d44f-7b4c-4720-ab79-499cb587202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z_sample = torch.randn(30, Z_DIM).to(DEVICE)\n",
    "    samples = vae.decoder(z_sample).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9043c1-53dd-430b-9d84-5bf08f773f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = vutils.make_grid(samples, nrow=10)\n",
    "plt.figure(figsize=(18,5))\n",
    "plt.imshow(grid.permute(1,2,0))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c303cf9-9799-45cc-9315-4623fc0f20e6",
   "metadata": {},
   "source": [
    "## 6. Manipulate the images <a name=\"manipulate\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819635b7-ab3c-4a80-83ef-ce00f0696b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the label dataset\n",
    "attributes = pd.read_csv(\"/app/data/celeba-dataset/list_attr_celeba.csv\")\n",
    "print(attributes.columns)\n",
    "attributes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f319d66-293b-4f38-8744-9e1dda150ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the face data with label attached\n",
    "LABEL = \"Blond_Hair\"  # <- Set this label\n",
    "labelled_test = utils.image_dataset_from_directory(\n",
    "    \"/app/data/celeba-dataset/img_align_celeba\",\n",
    "    labels=attributes[LABEL].tolist(),\n",
    "    color_mode=\"rgb\",\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    interpolation=\"bilinear\",\n",
    ")\n",
    "\n",
    "labelled = labelled_test.map(lambda x, y: (preprocess(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed6cc52-e0f7-465e-a197-8949dd9fbe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the attribute vector\n",
    "attribute_vec = get_vector_from_label(labelled, vae, Z_DIM, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88630224-8067-4510-86a0-2646068a4db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add vector to images\n",
    "add_vector_to_images(labelled, vae, attribute_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8f22fe-9601-4461-9473-72cb4ef80bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_faces(labelled, vae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
