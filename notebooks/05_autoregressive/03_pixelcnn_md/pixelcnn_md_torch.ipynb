{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b076bd1a-b236-4fbc-953d-8295b25122ae",
   "metadata": {},
   "source": [
    "# üëæ PixelCNN using PyTorch distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9235cbd1-f136-411c-88d9-f69f270c0b96",
   "metadata": {},
   "source": [
    "Âú®Ëøô‰∏™ Notebook ‰∏≠ÔºåÊàë‰ª¨Â∞Ü‰ΩøÁî® **PyTorch**ÔºåÂü∫‰∫é **Ê∑∑Âêà Logistic ÂàÜÂ∏ÉÔºàMixture of LogisticsÔºâ**Âú® **Fashion-MNIST** Êï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉ‰∏Ä‰∏™ PixelCNN„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e081e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84acc7be-6764-4668-b2bb-178f63deeed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e6268-ebd7-4feb-86db-1fe7abccdbe5",
   "metadata": {},
   "source": [
    "## 0. Parameters <a name=\"parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2ee6ce-129f-4833-b0c5-fa567381c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "N_COMPONENTS = 5      # Mixture of Logistics components\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7716fac-0010-49b0-b98e-53be2259edde",
   "metadata": {},
   "source": [
    "## 1. Prepare the data <a name=\"prepare\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebae2f0d-59fd-4796-841f-7213eae638de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Load Fashion-MNIST\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "x_train = train_dataset.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa53709f-7f3f-483b-9db8-2e5f9b9942c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(imgs):\n",
    "    imgs = np.expand_dims(imgs, 1)  # (N,1,H,W)\n",
    "    imgs = torch.tensor(imgs, dtype=torch.float32) / 255.0\n",
    "    imgs = F.interpolate(imgs, size=(IMAGE_SIZE, IMAGE_SIZE), mode=\"nearest\")\n",
    "    return imgs\n",
    "\n",
    "input_data = preprocess(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248737d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Display samples\n",
    "def display(images, n=10):\n",
    "    images = images[:n]\n",
    "    fig, axes = plt.subplots(1, n, figsize=(n * 1.5, 1.5))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(images[i, 0], cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "display(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff50401-3abe-4c10-bba8-b35bc13ad7d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Build the PixelCNN <a name=\"build\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a2a4a1-690e-4c94-b323-86f0e5b691d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, mask_type, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        assert mask_type in [\"A\", \"B\"]\n",
    "        self.register_buffer(\"mask\", torch.ones_like(self.weight))\n",
    "        _, _, h, w = self.weight.shape\n",
    "\n",
    "        self.mask[:, :, h // 2, w // 2 + (mask_type == \"B\"):] = 0\n",
    "        self.mask[:, :, h // 2 + 1:, :] = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        return super().forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1966e28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class PixelCNN(nn.Module):\n",
    "    def __init__(self, n_components):\n",
    "        super().__init__()\n",
    "        self.conv1 = MaskedConv2d(\"A\", 1, 64, kernel_size=7, padding=3)\n",
    "        self.conv2 = MaskedConv2d(\"B\", 64, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = MaskedConv2d(\"B\", 64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # ËæìÂá∫ÔºöÊØè‰∏™ÂÉèÁ¥†ÁöÑ mixture logits / means / scales\n",
    "        self.out = nn.Conv2d(\n",
    "            64,\n",
    "            n_components * 3,\n",
    "            kernel_size=1,\n",
    "        )\n",
    "\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b781a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def mixture_logistic_loss(x, params, n_components):\n",
    "    B, _, H, W = x.shape\n",
    "    params = params.view(B, n_components, 3, H, W)\n",
    "\n",
    "    logits = params[:, :, 0]\n",
    "    means = params[:, :, 1]\n",
    "    log_scales = torch.clamp(params[:, :, 2], min=-7)\n",
    "\n",
    "    x = x.unsqueeze(1)\n",
    "\n",
    "    centered = x - means\n",
    "    inv_std = torch.exp(-log_scales)\n",
    "    log_probs = centered * inv_std\n",
    "    log_probs = -log_probs - log_scales - 2 * F.softplus(-log_probs)\n",
    "\n",
    "    log_probs = log_probs + F.log_softmax(logits, dim=1)\n",
    "    return -torch.mean(torch.logsumexp(log_probs, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b14665-4359-447b-be58-3fd58ba69084",
   "metadata": {},
   "source": [
    "## 3. Train the PixelCNN <a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ec362d-41fa-473a-ad56-ebeec6cfd3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = PixelCNN(N_COMPONENTS).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "dataset = TensorDataset(input_data)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c525e44b-b3bb-489c-9d35-fcfe3e714e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for (x,) in loader:\n",
    "        x = x.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        params = model(x)\n",
    "        loss = mixture_logistic_loss(x, params, N_COMPONENTS)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | NLL: {total_loss / len(loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1f295f-ade0-4040-a6a5-a7b428b08ebc",
   "metadata": {},
   "source": [
    "## 4. Generate images <a name=\"generate\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db3cfe3-339e-463d-8af5-fbd403385fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "@torch.no_grad()\n",
    "def sample(model, n_samples):\n",
    "    model.eval()\n",
    "    images = torch.zeros(\n",
    "        n_samples, 1, IMAGE_SIZE, IMAGE_SIZE, device=device\n",
    "    )\n",
    "\n",
    "    for i in range(IMAGE_SIZE):\n",
    "        for j in range(IMAGE_SIZE):\n",
    "            params = model(images)\n",
    "            params = params.view(\n",
    "                n_samples, N_COMPONENTS, 3, IMAGE_SIZE, IMAGE_SIZE\n",
    "            )\n",
    "\n",
    "            logits = params[:, :, 0, i, j]\n",
    "            means = params[:, :, 1, i, j]\n",
    "\n",
    "            comp = torch.multinomial(\n",
    "                F.softmax(logits, dim=1), 1\n",
    "            ).squeeze()\n",
    "\n",
    "            images[:, 0, i, j] = means[\n",
    "                torch.arange(n_samples), comp\n",
    "            ]\n",
    "\n",
    "    return images.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80087297-3f47-4e0c-ac89-8758d4386d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "generated_images = sample(model, n_samples=2)\n",
    "display(generated_images, n=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
