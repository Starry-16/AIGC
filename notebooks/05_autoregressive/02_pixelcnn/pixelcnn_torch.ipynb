{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe3e4df",
   "metadata": {},
   "source": [
    "\n",
    "# ğŸ‘¾ PixelCNN from scratchï¼ˆPyTorch ç‰ˆæœ¬ï¼‰\n",
    "\n",
    "åœ¨è¿™ä¸ª Notebook ä¸­ï¼Œæˆ‘ä»¬å°†**ä»é›¶å¼€å§‹**ï¼Œä½¿ç”¨ **PyTorch** åœ¨ **Fashion-MNIST** æ•°æ®é›†ä¸Šè®­ç»ƒä¸€ä¸ª **PixelCNN** æ¨¡å‹ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42729ec",
   "metadata": {},
   "source": [
    "## 0. å‚æ•°ï¼ˆParametersï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe3672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMAGE_SIZE = 16          # è¾“å…¥å›¾åƒå¤§å°\n",
    "PIXEL_LEVELS = 4         # åƒç´ ç¦»æ•£çº§åˆ«\n",
    "N_FILTERS = 128          # å·ç§¯é€šé“æ•°\n",
    "RESIDUAL_BLOCKS = 5      # Residual Block æ•°é‡\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 150\n",
    "LR = 5e-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99bbd29",
   "metadata": {},
   "source": [
    "## 1. æ•°æ®å‡†å¤‡ï¼ˆPrepare the dataï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c92131",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c842a0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# åŠ è½½ Fashion-MNISTï¼ˆä»…ä½¿ç”¨è®­ç»ƒé›†å›¾åƒï¼‰\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "x_train = train_dataset.data.numpy()  # (N, 28, 28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b54b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# æ•°æ®é¢„å¤„ç†ï¼š\n",
    "# 1. resize åˆ° IMAGE_SIZE x IMAGE_SIZE\n",
    "# 2. ç¦»æ•£åŒ–åƒç´ åˆ° [0, PIXEL_LEVELS-1]\n",
    "# 3. å½’ä¸€åŒ–åˆ° [0, 1]\n",
    "\n",
    "def preprocess(imgs_int):\n",
    "    imgs_int = np.expand_dims(imgs_int, axis=1)  # (N,1,H,W)\n",
    "    imgs_int = torch.tensor(imgs_int, dtype=torch.float32)\n",
    "    imgs_int = F.interpolate(\n",
    "        imgs_int, size=(IMAGE_SIZE, IMAGE_SIZE), mode=\"nearest\"\n",
    "    )\n",
    "    imgs_int = (imgs_int / (256 / PIXEL_LEVELS)).long()\n",
    "    imgs = imgs_int.float() / PIXEL_LEVELS\n",
    "    return imgs, imgs_int\n",
    "\n",
    "input_data, output_data = preprocess(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211662f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# æ„å»º DataLoader\n",
    "dataset = TensorDataset(input_data, output_data.squeeze(1))\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d47ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# å¯è§†åŒ–éƒ¨åˆ†è®­ç»ƒæ ·æœ¬\n",
    "def display(images, n=10):\n",
    "    images = images[:n]\n",
    "    fig, axes = plt.subplots(1, n, figsize=(n * 1.5, 1.5))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(images[i, 0], cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "display(input_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dfe716",
   "metadata": {},
   "source": [
    "## 2. æ„å»º PixelCNN æ¨¡å‹ï¼ˆBuild the PixelCNNï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7b32f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, mask_type, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        assert mask_type in [\"A\", \"B\"]\n",
    "        self.mask_type = mask_type\n",
    "        self.register_buffer(\"mask\", torch.ones_like(self.weight))\n",
    "\n",
    "        _, _, h, w = self.weight.shape\n",
    "        self.mask[:, :, h // 2, w // 2 + (mask_type == \"B\") :] = 0\n",
    "        self.mask[:, :, h // 2 + 1 :, :] = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        return super().forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9949d82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, filters):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(filters, filters // 2, kernel_size=1)\n",
    "        self.conv2 = MaskedConv2d(\n",
    "            \"B\",\n",
    "            filters // 2,\n",
    "            filters // 2,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(filters // 2, filters, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.relu(self.conv3(out))\n",
    "        return x + out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5394527",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PixelCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_conv = MaskedConv2d(\n",
    "            \"A\", 1, N_FILTERS, kernel_size=7, padding=3\n",
    "        )\n",
    "\n",
    "        self.res_blocks = nn.ModuleList(\n",
    "            [ResidualBlock(N_FILTERS) for _ in range(RESIDUAL_BLOCKS)]\n",
    "        )\n",
    "\n",
    "        self.conv1 = MaskedConv2d(\"B\", N_FILTERS, N_FILTERS, kernel_size=1)\n",
    "        self.conv2 = MaskedConv2d(\"B\", N_FILTERS, N_FILTERS, kernel_size=1)\n",
    "\n",
    "        self.out_conv = nn.Conv2d(N_FILTERS, PIXEL_LEVELS, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input_conv(x))\n",
    "        for block in self.res_blocks:\n",
    "            x = block(x)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        return self.out_conv(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f601dfe",
   "metadata": {},
   "source": [
    "## 3. è®­ç»ƒ PixelCNNï¼ˆTrain the PixelCNNï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31c0d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = PixelCNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a23928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_images(model, num_img=10, temperature=1.0):\n",
    "    model.eval()\n",
    "    images = torch.zeros(\n",
    "        num_img, 1, IMAGE_SIZE, IMAGE_SIZE, device=device\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(IMAGE_SIZE):\n",
    "            for j in range(IMAGE_SIZE):\n",
    "                logits = model(images)[:, :, i, j]\n",
    "                probs = F.softmax(logits / temperature, dim=1)\n",
    "                samples = torch.multinomial(probs, 1).squeeze()\n",
    "                images[:, 0, i, j] = samples.float() / PIXEL_LEVELS\n",
    "\n",
    "    return images.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937b4297",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | Loss: {total_loss / len(loader):.4f}\")\n",
    "\n",
    "    # æ¯ä¸ª epoch ç”Ÿæˆæ ·æœ¬\n",
    "    samples = generate_images(model, num_img=10)\n",
    "    display(samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991ede54",
   "metadata": {},
   "source": [
    "## 4. å›¾åƒç”Ÿæˆï¼ˆGenerate imagesï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb3a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "samples = generate_images(model, num_img=10, temperature=1.0)\n",
    "display(samples)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
