{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b076bd1a-b236-4fbc-953d-8295b25122ae",
   "metadata": {},
   "source": [
    "# ü§™ Êù°‰ª∂ WGAN-GP Âú® CelebA ‰∫∫ËÑ∏Êï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9235cbd1-f136-411c-88d9-f69f270c0b96",
   "metadata": {},
   "source": [
    "Êú¨notebookÊºîÁ§∫Â¶Ç‰ΩïÂú® CelebA ‰∫∫ËÑ∏Êï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉËá™Â∑±ÁöÑÊù°‰ª∂ÁîüÊàêÂØπÊäóÁΩëÁªú (Conditional GAN, CGAN)„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cc98c1-83bf-4e5c-88d2-b3c55002dc65",
   "metadata": {},
   "source": [
    "‰ª£Á†ÅÊîπÁºñËá™ Sayak Paul ÁöÑ‰ºòÁßÄ [CGAN ÊïôÁ®ã](https://keras.io/examples/generative/conditional_gan/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84acc7be-6764-4668-b2bb-178f63deeed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e6268-ebd7-4feb-86db-1fe7abccdbe5",
   "metadata": {},
   "source": [
    "## 0. Parameters <a name=\"parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2ee6ce-129f-4833-b0c5-fa567381c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 64\n",
    "CHANNELS = 3\n",
    "CLASSES = 2\n",
    "BATCH_SIZE = 128\n",
    "Z_DIM = 32\n",
    "LEARNING_RATE = 5e-5\n",
    "ADAM_BETA_1 = 0.5\n",
    "ADAM_BETA_2 = 0.9\n",
    "EPOCHS = 20\n",
    "CRITIC_STEPS = 3\n",
    "GP_WEIGHT = 10.0\n",
    "LABEL = \"Blond_Hair\"\n",
    "DATA_PATH = \"/app/data/celeba-dataset\"\n",
    "LOAD_MODEL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7716fac-0010-49b0-b98e-53be2259edde",
   "metadata": {},
   "source": [
    "## 1. Prepare the data <a name=\"prepare\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8d9aad-59c1-46bf-9c37-d73bf90cc1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = pd.read_csv(os.path.join(DATA_PATH, \"list_attr_celeba.csv\"))\n",
    "labels = attributes[LABEL].tolist()\n",
    "int_labels = [1 if x == 1 else 0 for x in labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8876d4-a6a6-4a52-9b1e-cd985d43df20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, img_dir, labels, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_files = sorted(os.listdir(img_dir))\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_files[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)  # [-1, 1]\n",
    "])\n",
    "\n",
    "dataset = CelebADataset(\n",
    "    img_dir=os.path.join(DATA_PATH, \"img_align_celeba\"),\n",
    "    labels=int_labels,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff50401-3abe-4c10-bba8-b35bc13ad7d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Build the GAN <a name=\"build\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371eb69d-e534-4666-a412-b5b6fe24689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, img_channels, label_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(img_channels + label_dim, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(128, 128, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(128, 128, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(128, 1, 4, 1, 0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        # Â∞Ü one-hot label Êâ©Â±ïÂà∞ÂõæÂÉèÂ∞∫ÂØ∏Âπ∂ÊãºÊé•\n",
    "        label_map = labels[:, :, None, None].repeat(1, 1, x.size(2), x.size(3))\n",
    "        x = torch.cat([x, label_map], dim=1)\n",
    "        return self.model(x).view(x.size(0), -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086e2584-c60d-4990-89f4-2092c44e023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, label_dim, img_channels):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(z_dim + label_dim, 128*4*4)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose2d(128, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose2d(128, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose2d(64, img_channels, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        x = torch.cat([z, labels], dim=1)\n",
    "        x = self.fc(x).view(-1, 128, 4, 4)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88010f20-fb61-498c-b2b2-dac96f6c03b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = Critic(CHANNELS, CLASSES).to(device)\n",
    "generator = Generator(Z_DIM, CLASSES, CHANNELS).to(device)\n",
    "print(critic)\n",
    "print(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf2f892-9209-42ee-b251-1e7604df5335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, real, fake, labels):\n",
    "    batch_size = real.size(0)\n",
    "    alpha = torch.rand(batch_size, 1, 1, 1, device=device)\n",
    "    interpolated = (alpha * real + (1 - alpha) * fake).requires_grad_(True)\n",
    "    pred = critic(interpolated, labels)\n",
    "    grads = torch.autograd.grad(\n",
    "        outputs=pred,\n",
    "        inputs=interpolated,\n",
    "        grad_outputs=torch.ones_like(pred),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    grads = grads.view(grads.size(0), -1)\n",
    "    gp = ((grads.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b14665-4359-447b-be58-3fd58ba69084",
   "metadata": {},
   "source": [
    "## 3. Train the GAN <a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f48907-fa82-41b5-8caa-813b2f232c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_optimizer = torch.optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(ADAM_BETA_1, ADAM_BETA_2))\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(ADAM_BETA_1, ADAM_BETA_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b429fdad-ea9c-45a2-a556-eb950d793824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wgan_gp(epochs):\n",
    "    generator.train()\n",
    "    critic.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, (real_imgs, labels) in enumerate(tqdm(train_loader)):\n",
    "            real_imgs = real_imgs.to(device)\n",
    "            labels = F.one_hot(labels, num_classes=CLASSES).float().to(device)\n",
    "\n",
    "            # Critic Êõ¥Êñ∞Â§öÊ≠•\n",
    "            for _ in range(CRITIC_STEPS):\n",
    "                z = torch.randn(real_imgs.size(0), Z_DIM, device=device)\n",
    "                fake_imgs = generator(z, labels)\n",
    "\n",
    "                real_preds = critic(real_imgs, labels)\n",
    "                fake_preds = critic(fake_imgs.detach(), labels)\n",
    "\n",
    "                c_loss = fake_preds.mean() - real_preds.mean()\n",
    "                gp = gradient_penalty(critic, real_imgs, fake_imgs, labels)\n",
    "                total_c_loss = c_loss + GP_WEIGHT * gp\n",
    "\n",
    "                c_optimizer.zero_grad()\n",
    "                total_c_loss.backward()\n",
    "                c_optimizer.step()\n",
    "\n",
    "            # Generator Êõ¥Êñ∞\n",
    "            z = torch.randn(real_imgs.size(0), Z_DIM, device=device)\n",
    "            fake_imgs = generator(z, labels)\n",
    "            g_loss = -critic(fake_imgs, labels).mean()\n",
    "\n",
    "            g_optimizer.zero_grad()\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] | Critic Loss: {total_c_loss.item():.4f} | Generator Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "# %%\n",
    "train_wgan_gp(EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a75d5",
   "metadata": {},
   "source": [
    "## Generate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c525e44b-b3bb-489c-9d35-fcfe3e714e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(generator, num_images=10):\n",
    "    generator.eval()\n",
    "    z = torch.randn(num_images, Z_DIM, device=device)\n",
    "    labels_0 = F.one_hot(torch.zeros(num_images, dtype=torch.long), num_classes=CLASSES).float().to(device)\n",
    "    labels_1 = F.one_hot(torch.ones(num_images, dtype=torch.long), num_classes=CLASSES).float().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        imgs_0 = generator(z, labels_0).cpu()\n",
    "        imgs_1 = generator(z, labels_1).cpu()\n",
    "    \n",
    "    imgs_0 = (imgs_0 + 1) / 2  # [0,1]\n",
    "    imgs_1 = (imgs_1 + 1) / 2\n",
    "\n",
    "    grid_0 = utils.make_grid(imgs_0, nrow=5)\n",
    "    grid_1 = utils.make_grid(imgs_1, nrow=5)\n",
    "\n",
    "    utils.save_image(grid_0, \"./output/generated_label_0.png\")\n",
    "    utils.save_image(grid_1, \"./output/generated_label_1.png\")\n",
    "\n",
    "generate_images(generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
