{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b076bd1a-b236-4fbc-953d-8295b25122ae",
   "metadata": {},
   "source": [
    "# ðŸŽ¶ Music Generation with Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9235cbd1-f136-411c-88d9-f69f270c0b96",
   "metadata": {},
   "source": [
    "In this notebook, we'll walk through the steps required to train your own Transformer model to generate music in the style of the Bach cello suites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84acc7be-6764-4668-b2bb-178f63deeed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import music21\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e6268-ebd7-4feb-86db-1fe7abccdbe5",
   "metadata": {},
   "source": [
    "## 0. Parameters <a name=\"parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2ee6ce-129f-4833-b0c5-fa567381c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARSE_MIDI_FILES = True\n",
    "PARSED_DATA_PATH = \"./parsed_data/\"\n",
    "DATASET_REPETITIONS = 1\n",
    "\n",
    "SEQ_LEN = 50\n",
    "EMBEDDING_DIM = 256\n",
    "KEY_DIM = 256\n",
    "N_HEADS = 5\n",
    "DROPOUT_RATE = 0.3\n",
    "FEED_FORWARD_DIM = 256\n",
    "LOAD_MODEL = False\n",
    "\n",
    "EPOCHS = 5000\n",
    "BATCH_SIZE = 256\n",
    "GENERATE_LEN = 50\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4f5e63-e36a-4dc8-9f03-cb29c1fa5290",
   "metadata": {},
   "source": [
    "## 1. Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73de38bd-0b92-4441-9601-ed4a3b45f924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "file_list = glob.glob(\"./data/bach-cello/*.mid\")\n",
    "print(f\"Found {len(file_list)} MIDI files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ff575d-1632-43bf-844b-e5f2cea61454",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = music21.converter\n",
    "example_score = parser.parse(file_list[0]).chordify()\n",
    "example_score.show(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77459313-2417-4c18-b938-3a9859ec9bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "notes = [[\"C4\",\"E4\",\"G4\",\"C5\"]*20]*1000  # list of list of note strings\n",
    "durations = [[\"0.25\",\"0.25\",\"0.25\",\"0.25\"]*20]*1000  # list of list of duration strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f17b33-193e-4d83-8e0a-54dc8e7b249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_notes = notes[658]\n",
    "example_durations = durations[658]\n",
    "print(\"\\nNotes string\\n\", example_notes, \"...\")\n",
    "print(\"\\nDuration string\\n\", example_durations, \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba2f39c-882f-423a-88eb-334502413639",
   "metadata": {},
   "source": [
    "## 2. Tokenize the data <a name=\"tokenize\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244e093e-b977-4b1f-8f46-6503a55ea0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(sequences):\n",
    "    counter = Counter()\n",
    "    for seq in sequences:\n",
    "        counter.update(seq)\n",
    "    most_common = counter.most_common(5000)\n",
    "    itos = [\"<pad>\", \"<unk>\"] + [w for w,_ in most_common]\n",
    "    stoi = {w:i for i,w in enumerate(itos)}\n",
    "    return stoi, itos\n",
    "\n",
    "notes_stoi, notes_itos = build_vocab(notes)\n",
    "durations_stoi, durations_itos = build_vocab(durations)\n",
    "\n",
    "def tokenize_sequence(seq, stoi, max_len):\n",
    "    tokens = [stoi.get(s, stoi[\"<unk>\"]) for s in seq][:max_len+1]\n",
    "    x = tokens[:-1]\n",
    "    y = tokens[1:]\n",
    "    if len(x) < max_len:\n",
    "        pad_len = max_len - len(x)\n",
    "        x += [0]*pad_len\n",
    "        y += [0]*pad_len\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec4d36c-a4ad-4c32-89a2-749c21786441",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, notes, durations, notes_stoi, durations_stoi, seq_len):\n",
    "        self.notes = notes\n",
    "        self.durations = durations\n",
    "        self.notes_stoi = notes_stoi\n",
    "        self.durations_stoi = durations_stoi\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.notes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x_notes, y_notes = tokenize_sequence(self.notes[idx], self.notes_stoi, self.seq_len)\n",
    "        x_dur, y_dur = tokenize_sequence(self.durations[idx], self.durations_stoi, self.seq_len)\n",
    "        return torch.tensor(x_notes), torch.tensor(x_dur), torch.tensor(y_notes), torch.tensor(y_dur)\n",
    "\n",
    "dataset = MusicDataset(notes, durations, notes_stoi, durations_stoi, SEQ_LEN)\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823fb0c1-ebf8-453b-be94-9a33b466cae4",
   "metadata": {},
   "source": [
    "## 3. Causal mask <a name=\"create\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0f2a52-b157-478d-8de7-11ed6c383461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def causal_mask(seq_len):\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "    return mask  # shape: [seq_len, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52210a38-c8b4-4da7-90ce-eaa89c8fcafa",
   "metadata": {},
   "source": [
    "## 6. Create a Transformer Block layer <a name=\"transformer\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f25e8-4e3f-4849-9b92-676ea46e3ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=DROPOUT_RATE):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        mask = causal_mask(seq_len).to(x.device)\n",
    "        attn_out, attn_weights = self.attn(x, x, x, attn_mask=mask)\n",
    "        x = self.ln1(x + self.dropout(attn_out))\n",
    "        x = self.ln2(x + self.dropout(self.ff(x)))\n",
    "        return x, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ee1198-3bfc-415a-b247-02b8166133fe",
   "metadata": {},
   "source": [
    "## 7. Create the Token and Position Embedding <a name=\"embedder\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e30225-7fea-4a3c-afbc-b0017d5da661",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_len=SEQ_LEN):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, max_len, embed_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.token_emb(x)\n",
    "        x = x + self.pos_emb[:, :x.size(1), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee70157-5cb1-466c-bb9d-ba4720a93173",
   "metadata": {},
   "source": [
    "## 8. Build the Transformer model <a name=\"transformer_decoder\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df56f070-228b-4364-94ae-5b6a12349960",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicTransformer(nn.Module):\n",
    "    def __init__(self, notes_vocab_size, durations_vocab_size, embed_dim, num_heads, ff_dim):\n",
    "        super().__init__()\n",
    "        self.note_emb = TokenAndPositionEmbedding(notes_vocab_size, embed_dim//2)\n",
    "        self.dur_emb = TokenAndPositionEmbedding(durations_vocab_size, embed_dim//2)\n",
    "        self.transformer = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        self.fc_notes = nn.Linear(embed_dim, notes_vocab_size)\n",
    "        self.fc_dur = nn.Linear(embed_dim, durations_vocab_size)\n",
    "    \n",
    "    def forward(self, x_notes, x_dur):\n",
    "        note_e = self.note_emb(x_notes)\n",
    "        dur_e = self.dur_emb(x_dur)\n",
    "        x = torch.cat([note_e, dur_e], dim=-1)\n",
    "        x, attn = self.transformer(x)\n",
    "        note_out = self.fc_notes(x)\n",
    "        dur_out = self.fc_dur(x)\n",
    "        return note_out, dur_out, attn\n",
    "\n",
    "model = MusicTransformer(len(notes_itos), len(durations_itos), EMBEDDING_DIM, N_HEADS, FEED_FORWARD_DIM).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeab12c-b871-47c0-884c-752238b9f719",
   "metadata": {},
   "source": [
    "## 9. Train the Transformer <a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2003f19-610a-4acb-a225-0f6c1a3d3a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x_notes, x_dur, y_notes, y_dur in loader:\n",
    "        x_notes, x_dur, y_notes, y_dur = x_notes.to(DEVICE), x_dur.to(DEVICE), y_notes.to(DEVICE), y_dur.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        note_logits, dur_logits, _ = model(x_notes, x_dur)\n",
    "        loss = criterion(note_logits.view(-1, len(notes_itos)), y_notes.view(-1)) + \\\n",
    "               criterion(dur_logits.view(-1, len(durations_itos)), y_dur.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a82971-e609-4ced-9fd1-60eb9694c08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0445d3-5513-428b-995a-b337547d2a71",
   "metadata": {},
   "source": [
    "# 3. Generate music using the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682fcfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_logits(logits, temperature=1.0):\n",
    "    probs = F.softmax(logits / temperature, dim=-1)\n",
    "    token = torch.multinomial(probs, num_samples=1)\n",
    "    return token.item()\n",
    "\n",
    "def generate_music(model, start_notes, start_durations, max_tokens=50, temperature=0.5):\n",
    "    model.eval()\n",
    "    note_tokens = [notes_stoi.get(n, 1) for n in start_notes]\n",
    "    dur_tokens = [durations_stoi.get(d, 1) for d in start_durations]\n",
    "    generated_notes = note_tokens.copy()\n",
    "    generated_durs = dur_tokens.copy()\n",
    "    midi_stream = music21.stream.Stream()\n",
    "    midi_stream.append(music21.clef.BassClef())\n",
    "    for n,d in zip(start_notes, start_durations):\n",
    "        midi_stream.append(music21.note.Note(n) if n != \"START\" else music21.note.Rest())\n",
    "    for _ in range(max_tokens):\n",
    "        x_notes = torch.tensor([generated_notes[-SEQ_LEN:]], device=DEVICE)\n",
    "        x_dur = torch.tensor([generated_durs[-SEQ_LEN:]], device=DEVICE)\n",
    "        note_logits, dur_logits, _ = model(x_notes, x_dur)\n",
    "        next_note = sample_from_logits(note_logits[0,-1], temperature)\n",
    "        next_dur = sample_from_logits(dur_logits[0,-1], temperature)\n",
    "        generated_notes.append(next_note)\n",
    "        generated_durs.append(next_dur)\n",
    "        midi_stream.append(music21.note.Note(notes_itos[next_note]))\n",
    "    return midi_stream\n",
    "\n",
    "# %%\n",
    "midi_stream = generate_music(model, [\"START\"], [\"0.0\"], max_tokens=50)\n",
    "midi_stream.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
