{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b076bd1a-b236-4fbc-953d-8295b25122ae",
   "metadata": {},
   "source": [
    "# üöÄ GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dca6836-0007-43f3-af65-d12ae1922c02",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this notebook, we'll walk through the steps required to train your own GPT model on the wine review dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6cb7c7-d3d5-4b12-b357-1f6118edffe0",
   "metadata": {},
   "source": [
    "The code is adapted from the excellent [GPT tutorial](https://keras.io/examples/generative/text_generation_with_miniature_gpt/) created by Apoorv Nandan available on the Keras website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73350761-bef2-4e96-b3ac-a158eabd2b65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from IPython.display import display, HTML\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e6268-ebd7-4feb-86db-1fe7abccdbe5",
   "metadata": {},
   "source": [
    "## 0. Parameters <a name=\"parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8352af-343e-4c2e-8c91-95f8bac1c8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "MAX_LEN = 80\n",
    "EMBEDDING_DIM = 256\n",
    "KEY_DIM = 256\n",
    "N_HEADS = 2\n",
    "FEED_FORWARD_DIM = 256\n",
    "VALIDATION_SPLIT = 0.2\n",
    "SEED = 42\n",
    "LOAD_MODEL = False\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7716fac-0010-49b0-b98e-53be2259edde",
   "metadata": {},
   "source": [
    "## 1. Load the data <a name=\"load\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cf6b0f-9667-4146-8911-763a8a2925d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/app/data/wine-reviews/winemag-data-130k-v2.json\") as f:\n",
    "    wine_data = json.load(f)\n",
    "\n",
    "# %%\n",
    "# Filter dataset\n",
    "filtered_data = [\n",
    "    \"wine review : \"\n",
    "    + x[\"country\"]\n",
    "    + \" : \"\n",
    "    + x[\"province\"]\n",
    "    + \" : \"\n",
    "    + x[\"variety\"]\n",
    "    + \" : \"\n",
    "    + x[\"description\"]\n",
    "    for x in wine_data\n",
    "    if all(x[k] is not None for k in [\"country\", \"province\", \"variety\", \"description\"])\n",
    "]\n",
    "\n",
    "print(f\"{len(filtered_data)} entries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389c20de-0422-4c48-a7b4-6ee12a7bf0e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example entry\n",
    "example = filtered_data[25]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f871aaf-d873-41c7-8946-e4eef7ac17c1",
   "metadata": {},
   "source": [
    "## 2. Tokenize the data <a name=\"tokenize\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2064fb-5dcc-4657-b470-0928d10e2ddc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_punctuation(s):\n",
    "    s = re.sub(f\"([{string.punctuation}, '\\n'])\", r\" \\1 \", s)\n",
    "    s = re.sub(\" +\", \" \", s)\n",
    "    return s\n",
    "\n",
    "text_data = [pad_punctuation(x.lower()) for x in filtered_data]\n",
    "example_data = text_data[25]\n",
    "print(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87d7c65-9a46-492a-a5c0-a043b0d252f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Build vocabulary\n",
    "# ‰ΩøÁî®ËØçÈ¢ëÈÄâÊã©Ââç VOCAB_SIZE ‰∏™ÂçïËØç‰Ωú‰∏∫ËØçÊ±áË°®\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter()\n",
    "for line in text_data:\n",
    "    counter.update(line.split())\n",
    "\n",
    "most_common = counter.most_common(VOCAB_SIZE-2)\n",
    "itos = [\"<pad>\", \"<unk>\"] + [w for w, _ in most_common]  # index to string\n",
    "stoi = {w:i for i,w in enumerate(itos)}  # string to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9834f916-b21a-4104-acc9-f28d3bd7a8c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_to_tokens(text):\n",
    "    return [stoi.get(t, stoi[\"<unk>\"]) for t in text.split()]\n",
    "\n",
    "tokenized_data = [text_to_tokens(line) for line in text_data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c195efb-84c6-4be0-a989-a7542188ad35",
   "metadata": {},
   "source": [
    "## 3. Create the Training Set <a name=\"create\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c8aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "class WineDataset(Dataset):\n",
    "    def __init__(self, tokenized_data, max_len):\n",
    "        self.data = tokenized_data\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.data[idx]\n",
    "        tokens = tokens[:self.max_len+1]  # trim if longer\n",
    "        x = tokens[:-1]\n",
    "        y = tokens[1:]\n",
    "        # pad\n",
    "        if len(x) < self.max_len:\n",
    "            pad_len = self.max_len - len(x)\n",
    "            x = x + [0]*pad_len\n",
    "            y = y + [0]*pad_len\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "dataset = WineDataset(tokenized_data, MAX_LEN)\n",
    "train_size = int(len(dataset)*(1-VALIDATION_SPLIT))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff50401-3abe-4c10-bba8-b35bc13ad7d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Create the causal attention mask function <a name=\"causal\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eba2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(seq_len):\n",
    "    # mask[i,j] = True if j > i else False\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "    return mask  # shape: [seq_len, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3501dbad-0860-40ad-b7d6-47950e37858f",
   "metadata": {},
   "source": [
    "## 6. Create a Transformer Block layer <a name=\"transformer\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66af3d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, embed_dim]\n",
    "        seq_len = x.size(1)\n",
    "        mask = causal_mask(seq_len).to(x.device)\n",
    "        attn_output, attn_weights = self.attn(x, x, x, attn_mask=mask)\n",
    "        x = self.ln1(x + self.dropout(attn_output))\n",
    "        x = self.ln2(x + self.dropout(self.ff(x)))\n",
    "        return x, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076a6be0-9796-4974-9bcd-6ebbcfe7514e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7. Create the Token and Position Embedding <a name=\"embedder\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5cb25-88ae-4026-9e21-c1e6b5094a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = nn.Embedding(max_len, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
    "        return self.token_emb(x) + self.pos_emb(positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac2e2d4-5980-47e3-b5b0-6c41c0c2d152",
   "metadata": {},
   "source": [
    "## 8. Build the Transformer model <a name=\"transformer_decoder\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c57596e-e17d-4959-b6e8-7581b0bace3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, embed_dim, num_heads, ff_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = TokenAndPositionEmbedding(vocab_size, max_len, embed_dim)\n",
    "        self.transformer = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, attn = self.transformer(x)\n",
    "        logits = self.fc_out(x)\n",
    "        return logits, attn\n",
    "\n",
    "model = GPT(VOCAB_SIZE, MAX_LEN, EMBEDDING_DIM, N_HEADS, FEED_FORWARD_DIM).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b14665-4359-447b-be58-3fd58ba69084",
   "metadata": {},
   "source": [
    "## 9. Train the Transformer <a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddcff5f-829d-4449-99d2-9a3cb68f7d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(x)\n",
    "        loss = criterion(logits.view(-1, VOCAB_SIZE), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            logits, _ = model(x)\n",
    "            loss = criterion(logits.view(-1, VOCAB_SIZE), y.view(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# %%\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss = evaluate(model, val_loader, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - train_loss: {train_loss:.4f}, val_loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64e02d2-84dc-40c8-8446-40c09adf1e20",
   "metadata": {},
   "source": [
    "# 10. Generate text using the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28370864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_logits(logits, temperature=1.0):\n",
    "    probs = F.softmax(logits / temperature, dim=-1)\n",
    "    token = torch.multinomial(probs, num_samples=1)\n",
    "    return token.item()\n",
    "\n",
    "def generate_text(model, start_prompt, max_tokens=80, temperature=1.0):\n",
    "    model.eval()\n",
    "    tokens = [stoi.get(w, stoi[\"<unk>\"]) for w in start_prompt.split()]\n",
    "    generated = tokens.copy()\n",
    "    for _ in range(max_tokens):\n",
    "        x = torch.tensor(generated[-MAX_LEN:], device=DEVICE).unsqueeze(0)\n",
    "        logits, _ = model(x)\n",
    "        next_token = sample_from_logits(logits[0, -1], temperature)\n",
    "        if next_token == 0:\n",
    "            break\n",
    "        generated.append(next_token)\n",
    "    return \" \".join([itos[t] for t in generated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cef652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(model, \"wine review : us\", max_tokens=50, temperature=1.0))\n",
    "print(generate_text(model, \"wine review : italy\", max_tokens=50, temperature=0.5))\n",
    "print(generate_text(model, \"wine review : germany\", max_tokens=50, temperature=0.5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
