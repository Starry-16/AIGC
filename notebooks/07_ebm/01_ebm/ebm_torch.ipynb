{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b076bd1a-b236-4fbc-953d-8295b25122ae",
   "metadata": {},
   "source": [
    "# ⚡️ Energy-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9235cbd1-f136-411c-88d9-f69f270c0b96",
   "metadata": {},
   "source": [
    "In this notebook, we'll walk through the steps required to train your own Energy Based Model to predict the distribution of a demo dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2531aef5-c81a-4b53-a344-4b979dd4eec5",
   "metadata": {},
   "source": [
    "The code is adapted from the excellent ['Deep Energy-Based Generative Models' tutorial](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial8/Deep_Energy_Models.html) created by Phillip Lippe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84acc7be-6764-4668-b2bb-178f63deeed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fdc6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e6268-ebd7-4feb-86db-1fe7abccdbe5",
   "metadata": {},
   "source": [
    "## 0. Parameters <a name=\"parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2ee6ce-129f-4833-b0c5-fa567381c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "IMAGE_SIZE = 32\n",
    "CHANNELS = 1\n",
    "STEP_SIZE = 10\n",
    "STEPS = 60\n",
    "NOISE = 0.005\n",
    "ALPHA = 0.1\n",
    "GRADIENT_CLIP = 0.03\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 8192\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 60\n",
    "LOAD_MODEL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8281835",
   "metadata": {},
   "source": [
    "## 1. Prepare the data <a name=\"parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a73e5a4-1638-411c-8d3c-29f823424458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# %%\n",
    "# Load MNIST\n",
    "train_set = datasets.MNIST(root=\"./data\", train=True, download=True)\n",
    "test_set = datasets.MNIST(root=\"./data\", train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20697102-8c8d-4582-88d4-f8e2af84e060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def preprocess(imgs):\n",
    "    \"\"\"\n",
    "    Normalize and reshape the images\n",
    "    \"\"\"\n",
    "    imgs = imgs.astype(\"float32\")\n",
    "    imgs = (imgs - 127.5) / 127.5\n",
    "    imgs = np.pad(imgs, ((0, 0), (2, 2), (2, 2)), constant_values=-1.0)\n",
    "    imgs = np.expand_dims(imgs, axis=1)\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13668819-2e42-4661-8682-33ff2c24ae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "x_train = preprocess(train_set.data.numpy())\n",
    "x_test = preprocess(test_set.data.numpy())\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(torch.from_numpy(x_train)),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(torch.from_numpy(x_test)),\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e1a420-699e-4869-8d10-3c049dbad030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def display(images, n=10):\n",
    "    images = images[:n]\n",
    "    fig, axes = plt.subplots(1, n, figsize=(n * 1.2, 1.2))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(images[i, 0], cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# show a batch\n",
    "sample = next(iter(train_loader))[0]\n",
    "display(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53945d9-b7c5-49d0-a356-bcf1d1e1798b",
   "metadata": {},
   "source": [
    "## 2. Build the EBM network <a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8936d951-3281-4424-9cce-59433976bf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class EBMNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 5, stride=2, padding=2),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(64, 64, 3, stride=2, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 2 * 2, 64),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32221908-8819-48fa-8e57-0dc5179ca2cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = EBMNet().to(device)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    model.load_state_dict(torch.load(\"./models/model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f392424-45a9-49cc-8ea0-c1bec9064d74",
   "metadata": {},
   "source": [
    "## 3. Set up a Langevin sampler function <a name=\"sampler\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf10775a-0fbf-42df-aca5-be4b256a0c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def generate_samples(\n",
    "    model, imgs, steps, step_size, noise, return_img_per_step=False\n",
    "):\n",
    "    imgs = imgs.clone().detach().to(device)\n",
    "    imgs.requires_grad_(True)\n",
    "\n",
    "    imgs_per_step = []\n",
    "\n",
    "    for _ in range(steps):\n",
    "        imgs.data += noise * torch.randn_like(imgs)\n",
    "        imgs.data.clamp_(-1.0, 1.0)\n",
    "\n",
    "        energy = model(imgs).sum()\n",
    "        grads = torch.autograd.grad(energy, imgs)[0]\n",
    "        grads = torch.clamp(grads, -GRADIENT_CLIP, GRADIENT_CLIP)\n",
    "\n",
    "        imgs.data += step_size * grads\n",
    "        imgs.data.clamp_(-1.0, 1.0)\n",
    "\n",
    "        if return_img_per_step:\n",
    "            imgs_per_step.append(imgs.detach().cpu())\n",
    "\n",
    "    if return_img_per_step:\n",
    "        return torch.stack(imgs_per_step)\n",
    "    return imgs.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180fb0a1-ed16-47c2-b326-ad66071cd6e2",
   "metadata": {},
   "source": [
    "## 4. Set up a buffer to store examples <a name=\"buffer\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52615dcd-be2b-4e05-b729-0ec45ea6ef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class Buffer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.examples = [\n",
    "            torch.rand(1, 1, IMAGE_SIZE, IMAGE_SIZE) * 2 - 1\n",
    "            for _ in range(BATCH_SIZE)\n",
    "        ]\n",
    "\n",
    "    def sample_new_exmps(self, steps, step_size, noise):\n",
    "        n_new = np.random.binomial(BATCH_SIZE, 0.05)\n",
    "        rand_imgs = torch.rand(\n",
    "            n_new, 1, IMAGE_SIZE, IMAGE_SIZE\n",
    "        ) * 2 - 1\n",
    "\n",
    "        old_imgs = torch.cat(\n",
    "            random.choices(self.examples, k=BATCH_SIZE - n_new), dim=0\n",
    "        )\n",
    "\n",
    "        inp_imgs = torch.cat([rand_imgs, old_imgs], dim=0).to(device)\n",
    "\n",
    "        inp_imgs = generate_samples(\n",
    "            self.model, inp_imgs, steps, step_size, noise\n",
    "        )\n",
    "\n",
    "        self.examples = (\n",
    "            torch.split(inp_imgs.cpu(), 1, dim=0) + self.examples\n",
    "        )[:BUFFER_SIZE]\n",
    "\n",
    "        return inp_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a2a4a1-690e-4c94-b323-86f0e5b691d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class EBM:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.buffer = Buffer(model)\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=LEARNING_RATE\n",
    "        )\n",
    "\n",
    "    def train_step(self, real_imgs):\n",
    "        real_imgs = real_imgs.to(device)\n",
    "        real_imgs += NOISE * torch.randn_like(real_imgs)\n",
    "        real_imgs.clamp_(-1.0, 1.0)\n",
    "\n",
    "        fake_imgs = self.buffer.sample_new_exmps(\n",
    "            STEPS, STEP_SIZE, NOISE\n",
    "        )\n",
    "\n",
    "        inp_imgs = torch.cat([real_imgs, fake_imgs], dim=0)\n",
    "\n",
    "        real_out, fake_out = torch.chunk(\n",
    "            self.model(inp_imgs), 2, dim=0\n",
    "        )\n",
    "\n",
    "        cdiv_loss = fake_out.mean() - real_out.mean()\n",
    "        reg_loss = ALPHA * (real_out.pow(2).mean() + fake_out.pow(2).mean())\n",
    "        loss = cdiv_loss + reg_loss\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss.item(),\n",
    "            \"cdiv\": cdiv_loss.item(),\n",
    "            \"reg\": reg_loss.item(),\n",
    "            \"real\": real_out.mean().item(),\n",
    "            \"fake\": fake_out.mean().item(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6337e801-eb59-4abe-84dc-9536cf4dc257",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm = EBM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b14665-4359-447b-be58-3fd58ba69084",
   "metadata": {},
   "source": [
    "## 3. Train the EBM network <a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ec362d-41fa-473a-ad56-ebeec6cfd3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "for epoch in range(EPOCHS):\n",
    "    metrics = []\n",
    "    for (x,) in train_loader:\n",
    "        metrics.append(ebm.train_step(x))\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        avg = {k: np.mean([m[k] for m in metrics]) for k in metrics[0]}\n",
    "        print(f\"Epoch {epoch:03d} |\", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1f295f-ade0-4040-a6a5-a7b428b08ebc",
   "metadata": {},
   "source": [
    "## 4. Generate images <a name=\"generate\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db3cfe3-339e-463d-8af5-fbd403385fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "start_imgs = torch.rand(10, 1, IMAGE_SIZE, IMAGE_SIZE) * 2 - 1\n",
    "display(start_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80087297-3f47-4e0c-ac89-8758d4386d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "gen_imgs = generate_samples(\n",
    "    model,\n",
    "    start_imgs,\n",
    "    steps=1000,\n",
    "    step_size=STEP_SIZE,\n",
    "    noise=NOISE,\n",
    "    return_img_per_step=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf4b749-5f6e-4a12-863f-b0bbcd23549c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "display(gen_imgs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac707f6-0597-499c-9a52-7cade6724795",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "imgs = []\n",
    "for i in [0, 1, 3, 5, 10, 30, 50, 100, 300, 999]:\n",
    "    imgs.append(gen_imgs[i][6])\n",
    "\n",
    "display(torch.stack(imgs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
